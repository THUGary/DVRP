# API summary

Generated by `scripts/generate_api.py`

### /home/user0/DVRP-11.8/run.py

**Functions:**

- def build_env(cfg, planner_type)
- def run_episode(cfg, seed=<expr>, render=<expr>, fps=<expr>, planner=<expr>)
- def main()

### /home/user0/DVRP-11.8/configs.py

**Classes:**

- Config
  - def __post_init__(self)

**Functions:**

- def get_default_config()
- def get_param_combinations()
  - doc: Creates an iterator that yields all unique and valid combinations of parameters

### /home/user0/DVRP-11.8/environment/__init__.py

### /home/user0/DVRP-11.8/environment/env.py

**Classes:**

- EnvState
- GridEnvironment
  - doc: Grid world DVRP environment skeleton.
  - def __init__(self, width, height, num_agents, capacity, depot=<expr>, generator=<expr>, max_time=<expr>, expiry_penalty_scale=<expr>, switch_penalty_coef=<expr>)
  - def _full_capacity(self)
    - doc: Return the vehicle full capacity.
  - def reset(self, seed=<expr>)
  - def step(self, actions)
  - def _obs(self)
  - def num_agents(self)
  - def num_agents(self, n)
  - def _spawn_initial_agent_states(self, num_agents, seed)

### /home/user0/DVRP-11.8/training/__init__.py

> Top-level training package aggregating planner, generator, and adversarial training entrypoints.

### /home/user0/DVRP-11.8/training/planner/__init__.py

> Planner training entrypoints: data generation, supervised training, RL fine-tuning.

### /home/user0/DVRP-11.8/training/planner/train_rl_planner.py

**Classes:**

- ValueCritic(<expr>)
  - def __init__(self, d_model)
  - def forward(self, x)

**Functions:**

- def aggregate_state_embedding(enc_nodes, enc_depot, node_mask)
- def detach_feats(feats)
- def compute_returns(rewards, dones, gamma, device)
- def evaluate_sample(model, critic, sample, lateness_lambda, device)
- def ppo_update(model, critic, opt_policy, opt_value, decision_steps, returns_all, args, device, lateness_lambda)
- def build_env_from_cfg(cfg)
- def parse_args()
- def select_targets_with_sampling(model, feats, agents_tensor, lateness_lambda, cap_full, critic=<expr>)
  - doc: Sample a one-step target for each agent using softmax over logits.
- def main()

### /home/user0/DVRP-11.8/training/planner/train_model.py

**Classes:**

- PlanRowsDataset(<expr>)
  - doc: 读取 data_gen.py 生成的 rows 格式数据
  - def __init__(self, path)
  - def __len__(self)
  - def __getitem__(self, idx)

**Functions:**

- def collate_rows(batch)
  - doc: 将变长 nodes 的 row 批量化，支持多 agent 和 (A,K) 标签。
- def build_argparser()
- def save_ckpt(model, ckpt_dir, map_wid, agent_num, epoch)
- def evaluate(model, loader, device, lateness_lambda, amp, k)
- def main()

### /home/user0/DVRP-11.8/training/planner/data_gen.py

**Functions:**

- def _ensure_dir(path)
- def _to_builtin_number(x)
  - doc: 将 numpy 标量/torch 标量 转换为内置 Python number，其他类型原样返回。
- def _sanitize_for_torch_save(obj)
  - doc: 递归地将对象转换为 PyTorch 2.6+ 在默认 weights_only 加载下也安全/可兼容的结构：
- def _build_planner(planner_type, capacity=<expr>)
- def _unique_nodes_by_xy(observations)
  - doc: 去重：按 (x,y) 保留第一条，且忽略 c<=0 的点（与 RuleBasedPlanner 的可行性过滤一致）。
- def _target_to_label(target, nodes_unique)
  - doc: 将目标坐标映射为分类标签索引。约定 depot=0，nodes 为 1..N（对应 nodes_unique[0..N-1]）。
- def _targets_to_k_labels(tgt_deque, nodes_unique, k)
- def collect_rows_from_call(time_now, observations, agent_states_xyz, depot_xy, k, current_plans, global_nodes, serve_mark, unserved_count, targets, meta, full_capacity)
  - doc: 从一次 planner 调用组装一个包含所有 agents 的监督样本 row：labels 形状为 [A, K]。
- def generate_dataset(cfg, episodes, planner_type, seed, out_dir, val_ratio, replan_policy=<expr>, k=<expr>)
  - doc: 运行 episodes，收集 rows 并落盘
- def main()

### /home/user0/DVRP-11.8/training/generator/__init__.py

> Generator training entrypoints: data pipeline and diffusion training.

### /home/user0/DVRP-11.8/training/generator/generate_data.py

> This scirpt use the RuleBasedGenerator to generate a large dataset of DVRP episodes with varied parameters, saving the results to a CSV file for later use in training the neural network-based demand generator.

**Functions:**

- def generate_episode_data(args)
  - doc: Worker function to generate all data for a single episode.
- def generate_data_parallel(mini=<expr>)
  - doc: Main function to generate the dataset in parallel using multiple CPU cores.
- def parse_args()

### /home/user0/DVRP-11.8/training/generator/normalize_data.py

> This script normalizes the DVRP demand dataset by processing each episode in parallel.  It reads raw demand data from a CSV file, normalizes the demand features using predefined normalization functions,  and saves the processed dataset into a PyTorch .pt file for efficient loading during training.

**Functions:**

- def init_worker(cfg)
  - doc: Initializes worker process with global variables for map dimensions.
- def normalize_episode(episode_group)
  - doc: Worker function to normalize all data for a single episode.
- def process_data_parallel(input_file, output_file)
  - doc: Reads a raw dataset, normalizes it in parallel, and saves the result.

### /home/user0/DVRP-11.8/training/generator/train_rl_diffusion_generator.py

> RL-style adversarial training for the diffusion demand generator to MINIMIZE a chosen planner's reward.  Goal: Learn demand distribution parameters via conditional diffusion so that a fixed planner (greedy or DVRPNet model planner) obtains the lowest possible environment reward. We treat the generator (diffusion model) as a stochastic policy producing a set of demands for an episode. Reward signal: negative of episode cumulative reward returned by `GridEnvironment`.  Algorithm (REINFORCE-style on diffusion): 1. Sample K episodes. For each episode:    - Sample a latent noise z ~ N(0,1) and generate demands via diffusion conditioned on current generator params.    - Roll out the environment with the selected planner to obtain episode reward R_env.    - Define generator reward R_gen = - R_env. 2. For each episode, we compute standard diffusion noise-prediction loss L_diff = MSE(predicted_noise, true_noise). 3. Weight the loss by an advantage (here raw R_gen or normalized baseline-subtracted) to push distribution towards adversarial demands. 4. Update diffusion model parameters.  Simplifications: - We treat the entire demand set generation as one action; finer-grained sequential diffusion RL is future work. - Baseline uses exponential moving average of rewards to reduce variance.  Constraints: - Generated demands must respect config param ranges: time in [0, max_time-1], x,y in grid bounds, capacity in [1,max_c], lifetime in [min_lifetime,max_lifetime].   We enforce by clipping / rounding after un-normalization similarly to NetDemandGenerator.  CLI Example: python scripts/train_rl_diffusion_generator.py --episodes 50 --planner greedy --device cuda python scripts/train_rl_diffusion_generator.py --episodes 50 --planner model --planner_ckpt checkpoints/planner/planner_20_2_200.pt --device cuda  Outputs: - Updated diffusion model checkpoint at --out_ckpt. - CSV log with episode, env_reward, gen_reward.

**Functions:**

- def build_argparser()
- def _make_environment(cfg)
- def _init_planner(planner_type, cfg, device, ckpt_path)
- def _plan_episode(planner, env, demands, renderer=<expr>, fps=<expr>, save_frames_dir=<expr>, debug=<expr>)
  - doc: Inject demands then roll out a simple control loop with no sophisticated planner replanning every step.
- def _generate_demands(model, condition, params, device)
- def main()

### /home/user0/DVRP-11.8/training/generator/train_diffusion_generator.py

**Classes:**

- EpisodeDataset(<expr>)
  - def __init__(self, data)
  - def __len__(self)
  - def __getitem__(self, idx)

**Functions:**

- def build_argparser()
- def collate_fn(batch)
  - doc: Pads sequences to the max length in a batch.
- def main()

### /home/user0/DVRP-11.8/adversarial/__init__.py

### /home/user0/DVRP-11.8/adversarial/train_adversarial.py

**Functions:**

- def main()

### /home/user0/DVRP-11.8/adversarial/builders.py

**Functions:**

- def build_env(cfg=<expr>)
- def build_planner(planner_type, cfg, device, ckpt=<expr>)
- def build_diffusion(cfg, device, init_ckpt=<expr>, num_steps=<expr>)

### /home/user0/DVRP-11.8/adversarial/types.py

**Classes:**

- EpisodeResult
- GeneratorPolicy(<expr>)
  - def sample_demands(self, cfg, device)
- PlannerPolicy(<expr>)
  - def plan(self, obs_demands, agent_states, depot, t, horizon=<expr>)

### /home/user0/DVRP-11.8/adversarial/trainers.py

**Classes:**

- AdvConfig
- DiffusionAdversarialTrainer
  - def __init__(self, env, model, condition, cfg, device, adv_cfg)
  - def train(self, planner, episodes, renderer=<expr>, save_path=<expr>, seed=<expr>)

**Functions:**

- def _generate_demands(model, condition, params)
- def rollout_episode(planner, env, demands, renderer=<expr>, fps=<expr>)

### /home/user0/DVRP-11.8/agent/__init__.py

### /home/user0/DVRP-11.8/agent/planner/net_planner.py

**Classes:**

- NetPlanner(<expr>)
  - doc: Placeholder for a learned planner policy.
  - def plan(self, observations, agent_states, depot, t, horizon=<expr>, current_plans=<expr>, global_nodes=<expr>, serve_mark=<expr>, unserved_count=<expr>)

### /home/user0/DVRP-11.8/agent/planner/__init__.py

### /home/user0/DVRP-11.8/agent/planner/dcp_planner.py

**Classes:**

- DistributedCooperativePlanner(<expr>)
  - doc: Distributed Cooperative Planner (DCP) - 修改版
  - def __init__(self, **params)
  - def plan(self, observations, agent_states, depot, t, horizon=<expr>, current_plans=<expr>, global_nodes=<expr>, serve_mark=<expr>, unserved_count=<expr>)

### /home/user0/DVRP-11.8/agent/planner/fri_planner.py

**Classes:**

- FastReactiveInserter(<expr>)
  - doc: Fast Reactive Inserter (FRI) - 快速响应插入型规划器
  - def __init__(self, **params)
  - def plan(self, observations, agent_states, depot, t, horizon=<expr>, current_plans=<expr>, global_nodes=<expr>, serve_mark=<expr>, unserved_count=<expr>)
  - def init_plan(self, obs_map, agent_states, depot, current_time)
    - doc: Build initial solution using Clarke-Wright Savings algorithm with soft time-window penalty.

### /home/user0/DVRP-11.8/agent/planner/model_planner.py

**Classes:**

- ModelPlanner(<expr>)
  - doc: 使用学习模型进行动态规划的 Planner。
  - def __init__(self, d_model=<expr>, nhead=<expr>, nlayers=<expr>, time_plan=<expr>, lateness_lambda=<expr>, device=<expr>, full_capacity=<expr>, **params)
    - doc: lateness_lambda: 若 >0，会对 logits 添加 -lambda * lateness 的偏置（ETA>due 的软惩罚）
  - def load_from_ckpt(self, ckpt_path)
    - doc: 从 checkpoints 加载 DVRPNet 权重。
  - def plan(self, observations, agent_states, depot, t, horizon=<expr>, current_plans=<expr>, global_nodes=<expr>, serve_mark=<expr>, unserved_count=<expr>)
    - doc: 返回每个 agent 的目标队列（deque[(x,y), ...]）

### /home/user0/DVRP-11.8/agent/planner/rbso_planner.py

**Classes:**

- RepairBasedStabilityOptimizer(<expr>)
  - doc: Repair-based Stability Optimizer (RBSO)
  - def __init__(self, **params)
  - def plan(self, observations, agent_states, depot, t, horizon=<expr>, current_plans=<expr>, global_nodes=<expr>, serve_mark=<expr>, unserved_count=<expr>)
  - def _initial_planning(self, observations, agent_states, depot, current_time, obs_map)
  - def _rebuild_routes(self, nodes, agents, depot, current_time, obs_map)
  - def _compute_etas(self, start_pos, route_points, start_time)

### /home/user0/DVRP-11.8/agent/planner/base.py

**Classes:**

- AgentState
- BasePlanner(<expr>)
  - doc: Planner interface.
  - def __init__(self, **params)
  - def plan(self, observations, agent_states, depot, t, horizon=<expr>, current_plans=<expr>, global_nodes=<expr>, serve_mark=<expr>, unserved_count=<expr>)

### /home/user0/DVRP-11.8/agent/planner/rule_planner.py

**Classes:**

- RuleBasedPlanner(<expr>)
  - doc: Greedy planner with unique assignment per step and capacity feasibility.
  - def __init__(self, full_capacity=<expr>)
  - def plan(self, observations, agent_states, depot, t, horizon=<expr>, current_plans=<expr>, global_nodes=<expr>, serve_mark=<expr>, unserved_count=<expr>)

### /home/user0/DVRP-11.8/agent/controller/__init__.py

### /home/user0/DVRP-11.8/agent/controller/distance.py

> Compute travel time (traffic time) between two points.  Currently implemented as Manhattan distance (grid-based), but this is the single place to replace with a more realistic traffic-time model later.

**Functions:**

- def travel_time(a, b)
  - doc: Return the travel time between two grid points.

### /home/user0/DVRP-11.8/agent/controller/rule_controller.py

**Classes:**

- RuleBasedController(<expr>)
  - doc: Greedy controller: move one step toward the head of the target queue.
  - def act(self, current_pos, target_queue)

### /home/user0/DVRP-11.8/agent/controller/base.py

**Classes:**

- BaseController(<expr>)
  - doc: Controller interface: convert a target queue into a local move.
  - def __init__(self, **params)
  - def act(self, current_pos, target_queue)

### /home/user0/DVRP-11.8/agent/generator/__init__.py

### /home/user0/DVRP-11.8/agent/generator/rule_generator.py

**Classes:**

- Neighborhood
  - doc: Neighborhood for generating demands (2Dpositions, timestamps, quantities, lifetimes).
  - def __init__(self, center, rng, local_params, env_params, burst_params)
  - def sample(self, t)
    - doc: Sample demand points for current time step
  - def sample_one_xy(self)
    - doc: Sample a single (x,y) according to this neighborhood's configured distribution.
  - def _sample_poisson_process(self, max_time, lambda_param)
    - doc: Sample demand temporal points using Poisson process.
  - def _generate_demands(self, distribution, burst_mode=<expr>)
    - doc: Generate demands according to the specified distribution.
  - def merge_list_by_ids(self, A, B, A_pos)
  - def _basic_demands(self, distribution, time_series)
    - doc: Generate basic demands
  - def _burst_demand(self, distribution, time_series)
    - doc: Generate burst demands
  - def remove_in_depot(self, pts)
  - def _sample_uniform_2d(self, n_points, burst_mode=<expr>)
    - doc: sample uniform 2D points around the center
  - def _sample_gaussian_2d(self, n_points, burst_mode=<expr>)
    - doc: sample a 2D Gaussian point around the center
  - def _sample_cluster_2d(self, n_points, burst_mode=<expr>)
    - doc: sample points in 2D with exponential decay from center
  - def _sample_explosion_2d(self, n_points, burst_mode=<expr>)
    - doc: sample points in 2D with exponential decay from center
  - def _sample_implosion_2d(self, n_points, burst_mode=<expr>)
    - doc: sample points in 2D with exponential decay from center
- RuleBasedGenerator(<expr>)
  - doc: Generate demand points in rules.
  - def reset(self, seed=<expr>)
  - def _initialize_neighborhoods(self)
    - doc: Initialize concentrated generation areas
  - def sample(self, t)
    - doc: Sample all demand points at the current time step.
  - def _merge_demands_by_grid(self, demands)
    - doc: Merge demands fallen into the same grid cell.

### /home/user0/DVRP-11.8/agent/generator/data_utils.py

> Utility functions for preparing and normalizing conditional inputs for the demand generator.

**Functions:**

- def normalize_value(val, min_val, max_val)
  - doc: Normalizes a value from its original range to [-1, 1].
- def unnormalize_value(val, min_val, max_val)
  - doc: Un-normalizes a value from [-1, 1] to its original range [min_val, max_val].
- def prepare_condition(params)
  - doc: Creates the conditional input tensor from a dictionary of parameters.

### /home/user0/DVRP-11.8/agent/generator/base.py

**Classes:**

- Demand
- BaseDemandGenerator(<expr>)
  - doc: Interface for demand generators.
  - def __init__(self, width, height, **params)
  - def reset(self, seed=<expr>)
    - doc: Reset internal state. Implementations may use seed for RNG.
  - def sample(self, t)
    - doc: Return the list of new demands that appear at time t.

### /home/user0/DVRP-11.8/agent/generator/net_generator.py

**Classes:**

- NetDemandGenerator(<expr>)
  - doc: A demand generator that uses a trained neural network (e.g., a diffusion model)
  - def __init__(self, width, height, **params)
  - def reset(self, seed=<expr>)
    - doc: Loads the model and generates all demands for the entire episode.
  - def sample(self, t)
    - doc: Returns the pre-generated demands for the given time step `t`.
  - def _process_and_store_demands(self, tensor)
    - doc: Un-normalizes, validates, and converts the raw tensor output from the model 
  - def _relocate_from_depot(self, x, y)
    - doc: Find a nearby alternative cell when a demand collides with the depot.

### /home/user0/DVRP-11.8/scripts/generate_api.py

> Generate an API summary (Markdown) of Python files in the repository.  This script walks the workspace, parses .py files with the AST module, and extracts module docstrings, top-level classes (with methods) and top-level functions. It writes `api_summary.md` at the repository root.  Usage: python scripts/generate_api.py

**Functions:**

- def safe_unparse(node)
- def format_args(args)
- def summarize_file(path)
- def should_skip_dir(d)
- def main()

### /home/user0/DVRP-11.8/models/__init__.py

### /home/user0/DVRP-11.8/models/generator_model/__init__.py

### /home/user0/DVRP-11.8/models/generator_model/diffusion_model.py

**Classes:**

- SinusoidalPosEmb(<expr>)
  - doc: Helper module for sinusoidal time embeddings.
  - def __init__(self, dim)
  - def forward(self, time)
- MLPBlock(<expr>)
  - doc: A simple MLP block with SiLU activation and LayerNorm.
  - def __init__(self, dim_in, dim_out)
  - def forward(self, x)
- DemandDiffusionModel(<expr>)
  - doc: A conditional diffusion model for generating demand data.
  - def __init__(self, condition_dim, data_dim=<expr>, time_emb_dim=<expr>, num_steps=<expr>)
  - def q_sample(self, x_start, t, noise=<expr>)
    - doc: Forward process: add noise to data.
  - def predict_noise(self, x_t, t, condition)
    - doc: Predicts the noise added to x_t at timestep t.
  - def forward(self, x_start, condition)
    - doc: The training forward pass.
  - def sample(self, condition, num_demands, grid_size)
    - doc: The generation/sampling process (DDPM reverse process).

### /home/user0/DVRP-11.8/models/planner_model/__init__.py

### /home/user0/DVRP-11.8/models/planner_model/model.py

**Classes:**

- DVRPNet(<expr>)
  - doc:   - encode(feats)        仅编码 nodes/depot
  - def __init__(self, d_model=<expr>, nhead=<expr>, nlayers=<expr>)
  - def encode(self, feats)
    - doc: 仅编码 nodes/depot；返回:
  - def decode(self, enc_nodes, enc_depot, node_mask, agents_tensor, nodes=<expr>, lateness_lambda=<expr>)
    - doc: 仅执行解码一步：输入为 encode 后的张量与当前 agents 状态。
  - def _manhattan(a_xy, b_xy)
    - doc: a_xy: [...,2], b_xy: [...,2] -> Manhattan distance [...]
  - def forward(self, feats, agents, k, lateness_lambda=<expr>, cap_full)
    - doc: 流程：
  - def decode_step(self, feats, lateness_lambda=<expr>, current_time=<expr>)

**Functions:**

- def prepare_features(nodes, node_mask, depot, d_model=<expr>, device=<expr>)
  - doc: 将 Encoder 需要的量（nodes/node_mask/depot）转换为张量。
- def prepare_agents(agents, device=<expr>)
  - doc: 将 agents 转为张量（只做形状与 dtype 规整，不做特征工程）。

### /home/user0/DVRP-11.8/models/planner_model/encoder.py

**Classes:**

- Encoder(<expr>)
  - doc: 仅编码 nodes 与 depot 的 Encoder（不接收/处理 agents）。
  - def __init__(self, d_model=<expr>, nhead=<expr>, nlayers=<expr>)
  - def forward(self, feats)

### /home/user0/DVRP-11.8/models/planner_model/layers.py

**Classes:**

- MLP(<expr>)
  - def __init__(self, in_dim, out_dim, hidden=<expr>, act=<expr>)
  - def forward(self, x)
- TransformerBlock(<expr>)
  - def __init__(self, d_model, nhead, dim_ff=<expr>, dropout=<expr>)
  - def forward(self, x, key_padding_mask=<expr>, attn_mask=<expr>)
- CrossAttentionBlock(<expr>)
  - def __init__(self, d_model, nhead, dim_ff=<expr>, dropout=<expr>)
  - def forward(self, q, k, v, key_padding_mask=<expr>, attn_mask=<expr>)

### /home/user0/DVRP-11.8/models/planner_model/decoder.py

**Classes:**

- Decoder(<expr>)
  - doc: 解码器：
  - def __init__(self, d_model=<expr>, nhead=<expr>)
  - def forward(self, enc_nodes, enc_depot, node_mask, agents_tensor)

### /home/user0/DVRP-11.8/utils/state_manager.py

**Classes:**

- GlobalNodeList
  - doc: 全局节点列表，包含所有需求节点信息
  - def add_node(self, x, y, t, end_t, c)
    - doc: 添加新节点
  - def mark_served(self, x, y)
    - doc: 标记节点为已服务
  - def get_unserved_count(self)
    - doc: 获取未服务节点数量
  - def reset(self)
    - doc: 重置全局节点列表
- PlanningState
  - doc: 规划状态管理器
  - def reset(self, num_agents)
    - doc: 重置规划状态
  - def update_plans(self, new_plans)
    - doc: 更新规划结果
  - def get_unserved_count(self)
    - doc: 获取未服务节点数量

**Functions:**

- def update_planning_state(planning_state, agent_states, new_demands, obs_demands)
  - doc: 更新规划状态：

### /home/user0/DVRP-11.8/utils/__init__.py

### /home/user0/DVRP-11.8/utils/pygame_renderer.py

**Classes:**

- PygameRenderer
  - doc: Simple Pygame-based renderer for the DVRP grid environment.
  - def __init__(self, width, height, cell_size=<expr>, margin=<expr>, caption=<expr>)
  - def init(self)
  - def close(self)
  - def _handle_events(self)
  - def render(self, obs)
    - doc: Render one frame. Returns False if the window should close.
  - def _cell_rect(self, pos)
  - def _cell_center(self, pos)
  - def _draw_square(self, pos, color)
  - def _draw_circle(self, pos, color, radius_ratio=<expr>)
